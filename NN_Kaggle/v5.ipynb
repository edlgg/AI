{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def loadData():\n",
    "    train_raw = pd.read_csv(\"data/train.csv\", header=0).sample(frac=1) # Eduardo\n",
    "    test_raw = pd.read_csv(\"data/test.csv\", header=0) # Eduardo\n",
    "    store_raw = pd.read_csv(\"data/store.csv\", header=0) # Eduardo\n",
    "\n",
    "    train_raw.drop(\"Customers\", inplace=True, axis=1) # Drop customers colum because it is not present on test set.\n",
    "    test_raw.drop(\"Id\", inplace=True, axis=1) # Not relevant\n",
    "\n",
    "    test_raw[\"Sales\"] = [0] * len(test_raw)\n",
    "\n",
    "    return train_raw, test_raw, store_raw\n",
    "    \n",
    "train_raw, test_raw, store_raw = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class VoidTransformer(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    return\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "  def transform(self, X, y=None):\n",
    "    return X\n",
    "\n",
    "class DateSplitter(BaseEstimator, TransformerMixin):\n",
    "  def __init__(self):\n",
    "    return\n",
    "  def fit(self, X, y=None):\n",
    "    return self\n",
    "  def transform(self, X, y=None):\n",
    "    days = []\n",
    "    for date in X[\"Date\"]: # NOTE: Se podra paralelizar?\n",
    "      year, month, day = date.split(\"-\")\n",
    "      total = int(year) * 365 + int(month) * 30 + int(day)\n",
    "      days.append(total)\n",
    "    return np.c_[days]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pipeline = ColumnTransformer([\n",
    "                                  (\"nothing1\", VoidTransformer(),  [\"Store\"]),\n",
    "                                  (\"Categorical1\", OneHotEncoder(), [\"DayOfWeek\"]),\n",
    "                                  (\"date\", DateSplitter(), [\"Date\"]),\n",
    "                                  (\"nothing2\", VoidTransformer(), [\"Open\", \"Promo\"]),\n",
    "                                  # (\"Categorical2\", OneHotEncoder(), [\"StateHoliday\"]), # NOTE: Should one hot\n",
    "                                  (\"nothing3\", VoidTransformer(), [\"SchoolHoliday\", \"Sales\"]),\n",
    "\n",
    "])\n",
    "train_prepared = features_pipeline.fit_transform(train_raw)\n",
    "train_prepared = pd.DataFrame(train_prepared)\n",
    "train_prepared.rename(columns = {0:'Store', 12:'Sales', 8:'Days'}, inplace = True)\n",
    "\n",
    "test_prepared = features_pipeline.fit_transform(test_raw)\n",
    "test_prepared = pd.DataFrame(test_prepared)\n",
    "test_prepared.rename(columns = {0:'Store', 12:'Sales', 8:'Days'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_onehot_pipeline = Pipeline([\n",
    "            (\"nan_to_0\", SimpleImputer(strategy=\"constant\")),\n",
    "            (\"one_hot\", OneHotEncoder())                    \n",
    "])\n",
    "\n",
    "store_pipeline = ColumnTransformer([\n",
    "                                  (\"nothing1\", SimpleImputer(strategy='constant', fill_value=1), [\"Store\"]),\n",
    "                                  (\"Categorial1\", nan_onehot_pipeline, [\"StoreType\", \"Assortment\"]),\n",
    "                                  (\"CompetitionDistance\", SimpleImputer(strategy='mean'), [\"CompetitionDistance\"]),\n",
    "                                  (\"CompetitionSinceMonth\", SimpleImputer(strategy='constant'), [\"CompetitionOpenSinceMonth\"]),\n",
    "                                  (\"CompetitionSinceYear\", SimpleImputer(strategy='constant', fill_value=2016), [\"CompetitionOpenSinceYear\"]),\n",
    "                                  (\"nothing2\", VoidTransformer(), [\"Promo2\"]),\n",
    "                                  (\"Promo2SinceWeek\", SimpleImputer(strategy='constant'), [\"Promo2SinceWeek\"]),\n",
    "                                  (\"Promo2SinceYear\", SimpleImputer(strategy='constant', fill_value=2016), [\"Promo2SinceYear\"]),\n",
    "                                  (\"Categorical2\", nan_onehot_pipeline, [\"PromoInterval\"]),\n",
    "])\n",
    "\n",
    "stores_prepared = store_pipeline.fit_transform(store_raw)\n",
    "stores_prepared = pd.DataFrame(stores_prepared)\n",
    "stores_prepared.rename(columns = {0:'Store'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_prepared.merge(stores_prepared, on='Store', sort=False)\n",
    "test = test_prepared.merge(stores_prepared, on='Store', sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test[\"Sales\"]\n",
    "test_features = test.drop(\"Sales\", axis=1)\n",
    "train_labels = train[\"Sales\"]\n",
    "train_features = train.drop(\"Sales\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "x_scaled =  preprocessing.MinMaxScaler().fit_transform(train_features.values)\n",
    "train_features = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    # return K.sqrt(K.mean(K.square((y_true - y_pred) / K.clip(K.abs(y_true),K.epsilon(),None) ), axis=-1) )\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) / (y_true+1)), axis=-1) )\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train on 864627 samples, validate on 152582 samples\nEpoch 1/100\n864256/864627 [============================>.] - ETA: 0s - loss: 10105156.9195 - rmspe: 119.3275 - rmse: 2934.8130 - accuracy: 0.0656\nEpoch 00001: saving model to checkpoints/cp.ckpt\n864627/864627 [==============================] - 44s 51us/sample - loss: 10103457.3297 - rmspe: 119.3236 - rmse: 2934.6111 - accuracy: 0.0656 - val_loss: 6823901.1600 - val_rmspe: 84.0233 - val_rmse: 2197.3726 - val_accuracy: 0.0897\nEpoch 2/100\n864128/864627 [============================>.] - ETA: 0s - loss: 6054327.8943 - rmspe: 98.1943 - rmse: 2439.8879 - accuracy: 0.0774\nEpoch 00002: saving model to checkpoints/cp.ckpt\n864627/864627 [==============================] - 54s 62us/sample - loss: 6053551.7019 - rmspe: 98.1837 - rmse: 2439.7280 - accuracy: 0.0774 - val_loss: 6912392.4594 - val_rmspe: 76.1632 - val_rmse: 2180.8201 - val_accuracy: 0.1103\nEpoch 3/100\n279680/864627 [========>.....................] - ETA: 25s - loss: 5954986.0454 - rmspe: 91.5272 - rmse: 2419.9014 - accuracy: 0.0784"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, Sequential, callbacks\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from os import path\n",
    "\n",
    "train_features = pd.DataFrame(x_scaled)\n",
    "train_features = np.expand_dims(train_features, axis=-1)\n",
    "model = Sequential([\n",
    "              layers.Conv1D(filters=5, kernel_size=4, activation='relu', input_shape=(train_features.shape[1],1)),\n",
    "              layers.Flatten(),\n",
    "              layers.BatchNormalization(),\n",
    "              # layers.MaxPool1D(),  \n",
    "              layers.Dense(20, activation='relu'),\n",
    "              layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='mse',\n",
    "              metrics=[rmspe, rmse, 'accuracy'])\n",
    "\n",
    "checkpoint_path = \"checkpoints/cp.ckpt\"\n",
    "checkpoint_dir = path.dirname(checkpoint_path)\n",
    "cp_callback = callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "model.fit(train_features, train_labels.values, batch_size=128, validation_split=0.15, epochs=1, callbacks=[cp_callback])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.expand_dims(test_features, axis=-1)\n",
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standarisar datos\n",
    "# knn imputer\n",
    "# RMSPError\n",
    "# feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[2000:2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[2000:2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}